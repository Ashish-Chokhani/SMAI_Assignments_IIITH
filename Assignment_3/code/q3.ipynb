{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           CRIM          ZN      INDUS      CHAS       NOX        RM  \\\n",
      "mean   3.611874   11.211934  11.083992  0.069959  0.554695  6.284634   \n",
      "std    8.720192   23.388876   6.835896  0.255340  0.115878  0.702617   \n",
      "min    0.006320    0.000000   0.460000  0.000000  0.385000  3.561000   \n",
      "max   88.976200  100.000000  27.740000  1.000000  0.871000  8.780000   \n",
      "\n",
      "             AGE        DIS        RAD         TAX    PTRATIO           B  \\\n",
      "mean   68.518519   3.795043   9.549407  408.237154  18.455534  356.674032   \n",
      "std    27.999513   2.105710   8.707259  168.537116   2.164946   91.294864   \n",
      "min     2.900000   1.129600   1.000000  187.000000  12.600000    0.320000   \n",
      "max   100.000000  12.126500  24.000000  711.000000  22.000000  396.900000   \n",
      "\n",
      "          LSTAT       MEDV  \n",
      "mean  12.715432  22.532806  \n",
      "std    7.155871   9.197104  \n",
      "min    1.730000   5.000000  \n",
      "max   37.970000  50.000000  \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "#-------Load the dataset and Print the statistics-------------\n",
    "housing_data = pd.read_csv(\"HousingData.csv\")\n",
    "statistics = housing_data.agg(['mean', 'std', 'min', 'max'])\n",
    "print(statistics)\n",
    "\n",
    "#---a graph that shows the distribution of the various labels across the entire dataset----\n",
    "housing_medv = housing_data['MEDV']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(housing_medv, bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.xlabel('MEDV')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of MEDV')\n",
    "plt.show()\n",
    "\n",
    "#---------Split the data into train (70%), validation (15%), and test (15%) sets--------\n",
    "X = housing_data.drop(columns=['MEDV'])\n",
    "y = housing_data['MEDV']\n",
    "y=np.array(y)\n",
    "y = y[:, np.newaxis]\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "#-------------------Normalise and standarize the data------------------\n",
    "# Handle missing data (if any)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_val = imputer.transform(X_val)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Normalize the data using Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_val_normalized = scaler.transform(X_val)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "# Standardize the data (mean=0, std=1) using Z-score scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_standardized = scaler.fit_transform(X_train)\n",
    "X_val_standardized = scaler.transform(X_val)\n",
    "X_test_standardized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "\n",
    "class MLPRegressor:\n",
    "    def __init__(self, num_hidden_layers=1, num_neurons=64, activation='relu', learning_rate=0.01):\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_neurons = num_neurons\n",
    "        self.activation = activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.activations = [self.activation] * (num_hidden_layers + 1)\n",
    "        self.activations[-1] = 'linear'  # Use linear activation for regression\n",
    "\n",
    "    def initialize_weights_and_biases(self, input_dim):\n",
    "        layer_sizes = [input_dim] + [self.num_neurons] * self.num_hidden_layers + [1]  # Regression has one output\n",
    "        for i in range(1, len(layer_sizes)):\n",
    "            input_size, output_size = layer_sizes[i - 1], layer_sizes[i]\n",
    "            weight = np.random.randn(input_size, output_size) * 0.01\n",
    "            bias = np.zeros((1, output_size))\n",
    "            self.weights.append(weight)\n",
    "            self.biases.append(bias)\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        m = len(y_true)\n",
    "        loss = 0.5 * np.mean((y_true - y_pred) ** 2)\n",
    "        return loss\n",
    "\n",
    "    def activate(self, x, activation_type):\n",
    "        if activation_type == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        elif activation_type == 'sigmoid':\n",
    "            x = np.clip(x, -500, 500)\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif activation_type == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif activation_type == 'linear':\n",
    "            return x\n",
    "\n",
    "    def d_relu(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "    def d_sigmoid(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def d_tanh(self, x):\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "    \n",
    "    def d_linear(self, x):\n",
    "        return np.ones_like(x)\n",
    "\n",
    "    def forward_propagation(self, x):\n",
    "        z_values = []\n",
    "        a_values = [x]\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            z = np.dot(a_values[i], self.weights[i]) + self.biases[i]\n",
    "            a = self.activate(z, self.activations[i])\n",
    "            z_values.append(z)\n",
    "            a_values.append(a)\n",
    "\n",
    "        # Output layer with linear activation for regression\n",
    "        z = np.dot(a_values[-1], self.weights[-1]) + self.biases[-1]\n",
    "        a = self.activate(z, self.activations[-1])\n",
    "        z_values.append(z)\n",
    "        a_values.append(a)\n",
    "\n",
    "        return z_values, a_values\n",
    "\n",
    "    def backward_propagation(self, x, y):\n",
    "        m = x.shape[0]\n",
    "        z_values, a_values = self.forward_propagation(x)\n",
    "        gradients = [None] * (self.num_hidden_layers + 1)\n",
    "\n",
    "        # Compute gradient of the output layer\n",
    "        gradients[-1] = (a_values[-1] - y) / m\n",
    "\n",
    "        # Backpropagate through hidden layers\n",
    "        for i in reversed(range(self.num_hidden_layers)):\n",
    "            if self.activations[i] == 'relu':\n",
    "                gradients[i] = np.dot(gradients[i + 1], self.weights[i + 1].T) * self.d_relu(a_values[i + 1])\n",
    "            elif self.activations[i] == 'sigmoid':\n",
    "                gradients[i] = np.dot(gradients[i + 1], self.weights[i + 1].T) * self.d_sigmoid(a_values[i + 1])\n",
    "            elif self.activations[i] == 'tanh':\n",
    "                gradients[i] = np.dot(gradients[i + 1], self.weights[i + 1].T) * self.d_tanh(a_values[i + 1])\n",
    "            elif self.activations[i] == 'linear':\n",
    "                gradients[i] = np.dot(gradients[i + 1], self.weights[i + 1].T) * self.d_linear(a_values[i + 1])\n",
    "\n",
    "        # Compute gradients for weights and biases\n",
    "        dW = [None] * (self.num_hidden_layers + 1)\n",
    "        db = [None] * (self.num_hidden_layers + 1)\n",
    "        for i in range(self.num_hidden_layers + 1):\n",
    "            dW[i] = np.dot(a_values[i].T, gradients[i])\n",
    "            db[i] = np.sum(gradients[i], axis=0)\n",
    "\n",
    "        return dW, db\n",
    "\n",
    "    def predict(self, x):\n",
    "        _, y_pred = self.forward_propagation(x)\n",
    "        return y_pred[-1]\n",
    "\n",
    "    def sgd(self, x, y, num_epochs=1000):\n",
    "        input_dim = x.shape[1]\n",
    "        self.initialize_weights_and_biases(input_dim)\n",
    "\n",
    "        m = x.shape[0]  # Number of training samples\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for i in range(m):\n",
    "                # Select a random training sample (Stochastic Gradient Descent)\n",
    "                random_index = np.random.randint(0, m)\n",
    "                x_sample = x[random_index:random_index + 1]\n",
    "                y_sample = y[random_index:random_index + 1]\n",
    "\n",
    "                dW, db = self.backward_propagation(x_sample, y_sample)\n",
    "\n",
    "                for j in range(self.num_hidden_layers + 1):\n",
    "                    self.weights[j] -= self.learning_rate * dW[j]\n",
    "                    self.biases[j] -= self.learning_rate * db[j]\n",
    "\n",
    "                _, y_pred = self.forward_propagation(x)\n",
    "                loss = self.compute_loss(y, y_pred[-1])\n",
    "                wandb.log({f\"Loss Optimizer=SGD Epochs={num_epochs} Learning Rate={self.learning_rate} Num_hidden_layers={self.num_hidden_layers} Num_neurons={self.num_neurons} Activation={self.activation}\": loss})\n",
    "\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"SGD Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "        print(\"SGD Training complete!\")\n",
    "        \n",
    "    def batch_gradient_descent(self, x, y, batch_size=32, num_epochs=1000):\n",
    "        input_dim = x.shape[1]\n",
    "        self.initialize_weights_and_biases(input_dim)\n",
    "        \n",
    "        m = x.shape[0]  # Number of training samples\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            for i in range(0, m, batch_size):\n",
    "                x_batch = x[i:i+batch_size]\n",
    "                y_batch = y[i:i+batch_size]\n",
    "                \n",
    "                dW, db = self.backward_propagation(x_batch, y_batch)\n",
    "                \n",
    "                for j in range(self.num_hidden_layers + 1):\n",
    "                    self.weights[j] -= self.learning_rate * dW[j]\n",
    "                    self.biases[j] -= self.learning_rate * db[j]\n",
    "            \n",
    "            _, y_pred = self.forward_propagation(x)\n",
    "            loss = self.compute_loss(y, y_pred[-1])\n",
    "            wandb.log({f\"Loss Optimizer=Batch Epochs={num_epochs} Learning Rate={self.learning_rate} Num_hidden_layers={self.num_hidden_layers} Num_neurons={self.num_neurons} Activation={self.activation}\": loss})\n",
    "            \n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Batch Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        print(\"Batch Training complete!\")\n",
    "\n",
    "    def mini_batch_gradient_descent(self, x, y, batch_size=32, num_epochs=1000):\n",
    "        input_dim = x.shape[1]\n",
    "        self.initialize_weights_and_biases(input_dim)\n",
    "        \n",
    "        m = x.shape[0]  # Number of training samples\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            indices = np.arange(m)\n",
    "            np.random.shuffle(indices)\n",
    "            x_shuffled = x[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            for i in range(0, m, batch_size):\n",
    "                x_batch = x_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "                \n",
    "                dW, db = self.backward_propagation(x_batch, y_batch)\n",
    "                \n",
    "                for j in range(self.num_hidden_layers + 1):\n",
    "                    self.weights[j] -= self.learning_rate * dW[j]\n",
    "                    self.biases[j] -= self.learning_rate * db[j]\n",
    "            \n",
    "            _, y_pred = self.forward_propagation(x)\n",
    "            loss = self.compute_loss(y, y_pred[-1])\n",
    "            wandb.log({f\"Loss Optimizer=Mini_Batch Epochs={num_epochs} Learning Rate={self.learning_rate} Num_hidden_layers={self.num_hidden_layers} Num_neurons={self.num_neurons} Activation={self.activation}\": loss})\n",
    "            \n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Mini_Batch Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        print(\"Mini_Batch Training complete!\")\n",
    "    \n",
    "    def evaluate_model(self,x,y):\n",
    "        _, y_pred = self.forward_propagation(x)\n",
    "        loss = self.compute_loss(y, y_pred[-1])\n",
    "        wandb.log({f\"Loss Optimizer=Mini_Batch Learning Rate={self.learning_rate} Num_hidden_layers={self.num_hidden_layers} Num_neurons={self.num_neurons} Activation={self.activation}\": loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:xbakogee) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Loss Optimizer=Mini_Batch Learning Rate=0.01 Num_hidden_layers=2 Num_neurons=5 Activation=sigmoid</td><td>▁</td></tr><tr><td>Loss Optimizer=SGD Epochs=1000 Learning Rate=0.01 Num_hidden_layers=2 Num_neurons=5 Activation=sigmoid</td><td>█▅▄▂▂▃▂▂▂▃▃▄▂▂▂▂▁▁▁▁▁▁▁▃▃▃▃▃▃▂▃▃▃▂▃▂▃▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Loss Optimizer=Mini_Batch Learning Rate=0.01 Num_hidden_layers=2 Num_neurons=5 Activation=sigmoid</td><td>5.06754</td></tr><tr><td>Loss Optimizer=SGD Epochs=1000 Learning Rate=0.01 Num_hidden_layers=2 Num_neurons=5 Activation=sigmoid</td><td>5.39037</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">different-serenity-17</strong> at: <a href='https://wandb.ai/ashishchokhani2910/regression_example/runs/xbakogee' target=\"_blank\">https://wandb.ai/ashishchokhani2910/regression_example/runs/xbakogee</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231023_012048-xbakogee/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:xbakogee). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/ashishchokhani/Desktop/codes/SMAI/2021102016_Assignment_3/wandb/run-20231023_012457-noj74ass</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ashishchokhani2910/regression_example/runs/noj74ass' target=\"_blank\">autumn-surf-18</a></strong> to <a href='https://wandb.ai/ashishchokhani2910/regression_example' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ashishchokhani2910/regression_example' target=\"_blank\">https://wandb.ai/ashishchokhani2910/regression_example</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ashishchokhani2910/regression_example/runs/noj74ass' target=\"_blank\">https://wandb.ai/ashishchokhani2910/regression_example/runs/noj74ass</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Epoch 100/1000, Loss: 4.4626\n",
      "SGD Epoch 200/1000, Loss: 3.7785\n",
      "SGD Epoch 300/1000, Loss: 3.5614\n",
      "SGD Epoch 400/1000, Loss: 3.6114\n",
      "SGD Epoch 500/1000, Loss: 3.9872\n",
      "SGD Epoch 600/1000, Loss: 3.6247\n",
      "SGD Epoch 700/1000, Loss: 3.3578\n",
      "SGD Epoch 800/1000, Loss: 3.3101\n",
      "SGD Epoch 900/1000, Loss: 3.0701\n",
      "SGD Epoch 1000/1000, Loss: 3.2146\n",
      "SGD Training complete!\n",
      "Hyperparameters for the Best Model:\n",
      "learning_rate: 0.01\n",
      "num_epochs: 1000\n",
      "num_hidden_layers: 2\n",
      "num_neurons: 5\n",
      "activation: sigmoid\n",
      "optimizer: sgd\n",
      "+---------------+------------------+-------------------------+-------------------+---------------------+-----------+--------------------+-------------------+--------------------+\n",
      "| Learning Rate | Number of Epochs | Number of Hidden Layers | Number of Neurons | Activation Function | Optimizer |        MSE         |        RMSE       |     R-Squared      |\n",
      "+---------------+------------------+-------------------------+-------------------+---------------------+-----------+--------------------+-------------------+--------------------+\n",
      "|      0.01     |       1000       |            2            |         5         |       sigmoid       |    sgd    | 12.634104685227234 | 3.554448576815711 | 0.8178735803621102 |\n",
      "+---------------+------------------+-------------------------+-------------------+---------------------+-----------+--------------------+-------------------+--------------------+\n",
      "Regression Metrics on Test Set:\n",
      "MSE: 27.2820\n",
      "RMSE: 5.2232\n",
      "R-squared: 0.6567\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Loss Optimizer=Mini_Batch Learning Rate=0.01 Num_hidden_layers=2 Num_neurons=5 Activation=sigmoid</td><td>▁</td></tr><tr><td>Loss Optimizer=SGD Epochs=1000 Learning Rate=0.01 Num_hidden_layers=2 Num_neurons=5 Activation=sigmoid</td><td>█▅▄▃▂▂▂▂▂▂▂▂▂▂▁▂▂▃▄▁▁▂▃▂▂▂▁▂▂▂▁▁▁▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Loss Optimizer=Mini_Batch Learning Rate=0.01 Num_hidden_layers=2 Num_neurons=5 Activation=sigmoid</td><td>6.31705</td></tr><tr><td>Loss Optimizer=SGD Epochs=1000 Learning Rate=0.01 Num_hidden_layers=2 Num_neurons=5 Activation=sigmoid</td><td>3.21456</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">autumn-surf-18</strong> at: <a href='https://wandb.ai/ashishchokhani2910/regression_example/runs/noj74ass' target=\"_blank\">https://wandb.ai/ashishchokhani2910/regression_example/runs/noj74ass</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231023_012457-noj74ass/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from itertools import product\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Initialize and train the model\n",
    "wandb.init(project=\"regression_example\", entity=\"ashishchokhani2910\")\n",
    "best_mse = float('inf')\n",
    "best_model = None\n",
    "best_hyperparameters = None\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "learning_rates = [0.01, 0.001]\n",
    "num_epochs = [500, 1000]\n",
    "num_hidden_layers = [1, 2]\n",
    "num_neurons = [64, 128]\n",
    "activations = ['relu', 'sigmoid','tanh']\n",
    "optimizers = ['sgd', 'batch_gradient_descent', 'mini_batch_gradient_descent']\n",
    "\n",
    "# # Define hyperparameters to tune\n",
    "# learning_rates = [0.01]\n",
    "# num_epochs = [1000]\n",
    "# num_hidden_layers = [2]\n",
    "# num_neurons = [5]\n",
    "# activations = ['sigmoid']\n",
    "# optimizers = ['sgd']\n",
    "\n",
    "metrics_table = PrettyTable()\n",
    "metrics_table.field_names = [\"Learning Rate\", \"Number of Epochs\", \"Number of Hidden Layers\", \"Number of Neurons\", \"Activation Function\", \"Optimizer\",\"MSE\",\"RMSE\",\"R-Squared\"]\n",
    "\n",
    "\n",
    "# Initialize a dictionary to collect metrics and hyperparameters\n",
    "hyperparameter_metrics = {\n",
    "    'learning_rate': [],\n",
    "    'num_epochs': [],\n",
    "    'num_hidden_layers': [],\n",
    "    'num_neurons': [],\n",
    "    'activation': [],\n",
    "    'optimizer': [],\n",
    "    'MSE': [],\n",
    "    'RMSE': [],\n",
    "    'R2': [],\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "for lr, epochs, num_layers, num_neurons, activation, optimizer in product(learning_rates, num_epochs, num_hidden_layers, num_neurons, activations, optimizers):\n",
    "    \n",
    "    config = {\n",
    "        'learning_rate': lr,\n",
    "        'num_epochs': epochs,\n",
    "        'num_hidden_layers': num_layers,\n",
    "        'num_neurons': num_neurons,\n",
    "        'activation': activation,\n",
    "        'optimizer': optimizer\n",
    "    }\n",
    "    wandb.config.update(config)\n",
    "\n",
    "    model = MLPRegressor(\n",
    "        num_hidden_layers=num_layers,\n",
    "        num_neurons=num_neurons,\n",
    "        activation=activation,\n",
    "        learning_rate=lr,\n",
    "    )\n",
    "    model.sgd(X_train_standardized, y_train, num_epochs=epochs)\n",
    "    model.evaluate_model(X_val_standardized,y_val)\n",
    "    y_pred_val = model.predict(X_val_standardized)\n",
    "    \n",
    "    #Calculate and report metrics\n",
    "    mse = mean_squared_error(y_val, y_pred_val)\n",
    "    rmse = math.sqrt(mse)\n",
    "    r2 = r2_score(y_val, y_pred_val)\n",
    "    \n",
    "    if mse < best_mse:  # We want to minimize MSE\n",
    "        best_mse = mse\n",
    "        best_model = model\n",
    "        best_hyperparameters = {\n",
    "            'learning_rate': lr,\n",
    "            'num_epochs': epochs,\n",
    "            'num_hidden_layers': num_layers,\n",
    "            'num_neurons': num_neurons,\n",
    "            'activation': activation,\n",
    "            'optimizer': optimizer\n",
    "        }\n",
    "        \n",
    "    metrics_table.add_row([lr, epochs, num_layers, num_neurons, activation, optimizer, mse, rmse, r2])\n",
    "    \n",
    "    hyperparameter_metrics['learning_rate'].append(lr)\n",
    "    hyperparameter_metrics['num_epochs'].append(epochs)\n",
    "    hyperparameter_metrics['num_hidden_layers'].append(num_layers)\n",
    "    hyperparameter_metrics['num_neurons'].append(num_neurons)\n",
    "    hyperparameter_metrics['activation'].append(activation)\n",
    "    hyperparameter_metrics['optimizer'].append(optimizer)\n",
    "    hyperparameter_metrics['MSE'].append(mse)\n",
    "    hyperparameter_metrics['RMSE'].append(rmse)\n",
    "    hyperparameter_metrics['R2'].append(r2)\n",
    "\n",
    "\n",
    "print(\"Hyperparameters for the Best Model:\")\n",
    "for param, value in best_hyperparameters.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "\n",
    "print(metrics_table)\n",
    "\n",
    "y_pred_test = best_model.predict(X_test_standardized)\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "rmse_test = math.sqrt(mse_test)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "report = f\"MSE: {mse_test:.4f}\\nRMSE: {rmse_test:.4f}\\nR-squared: {r2_test:.4f}\"\n",
    "\n",
    "print(\"Regression Metrics on Test Set:\")\n",
    "print(report)\n",
    "\n",
    "\n",
    "# Finish the W&B run\n",
    "wandb.run.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W&B report for above hyperparameters\n",
    "\n",
    "https://wandb.ai/ashishchokhani2910/q3/reports/Performance-of-MLP-on-Regression--Vmlldzo1NzU2Mjc3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
